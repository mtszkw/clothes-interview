{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mobilenet_classification_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhb5rqVSBqZH6zHysprcHw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtszkw/kleidung/blob/main/mobilenet_classification_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URFMH2FHSb20"
      },
      "source": [
        "# !pip show pandas numpy matplotlib tensorflow optuna albumentations\n",
        "!pip install --quiet tensorflow==2.4.1 matplotlib==3.2.2 pandas==1.1.5 numpy==1.19.5 albumentations==0.1.12"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frljmpztSUd7",
        "outputId": "e81f6de2-0ec5-4034-b6c9-5c278c667031"
      },
      "source": [
        "# https://github.com/alexeygrigorev/clothing-dataset-small\n",
        "!git clone https://github.com/alexeygrigorev/clothing-dataset-small"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'clothing-dataset-small' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MwXvG-hTUYe"
      },
      "source": [
        "import os\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import seaborn as sns"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiu9VH7gwiJ3"
      },
      "source": [
        "class Config:\n",
        "    seed = 2020\n",
        "    batch_size = 64\n",
        "    img_width = 224\n",
        "    img_height = 224\n",
        "    num_classes = 10\n",
        "    num_epochs = 5\n",
        "    init_lr = 2e-3\n",
        "    dataset_directory = \"./clothing-dataset-small/\"\n",
        "    checkpt_directory = \"/content/mobilenet_fashion_clothes_clf/\"\n",
        "    checkpt_tar_path = \"/content/mobilenet_fashion_clothes_clf.tar.gz\"\n",
        "\n",
        "config = Config()"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuHaBzwvUMte",
        "outputId": "f2d1790b-4dbc-4985-8b63-ae1219da2a67"
      },
      "source": [
        "\"\"\"\n",
        "    Converts image pixel values to [0, 255] range.\n",
        "    This seems to be necessary for this dataset as\n",
        "    MobileNetV2 doesn't handle [-1, 1] range properly.\n",
        "\"\"\"\n",
        "def preprocess_func(img):\n",
        "    img = cv2.normalize(\n",
        "        img, None, alpha=0, beta=255,\n",
        "        norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "transforms = {\n",
        "    'rotation_range': 20,\n",
        "    'width_shift_range': 0.2,\n",
        "    'height_shift_range': 0.2,\n",
        "    'zoom_range': 0.2,\n",
        "    'horizontal_flip': True,\n",
        "}\n",
        "\n",
        "train_gen = ImageDataGenerator(preprocessing_function=preprocess_func, **transforms)\n",
        "valid_gen = ImageDataGenerator(preprocessing_function=preprocess_func, **transforms)\n",
        "\n",
        "train_ds = train_gen.flow_from_directory(\n",
        "    os.path.join(dataset_directory, \"train/\"),\n",
        "    seed=config.seed,\n",
        "    target_size=(config.img_width, config.img_height),\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True)\n",
        "\n",
        "val_ds = valid_gen.flow_from_directory(\n",
        "    os.path.join(dataset_directory, \"validation/\"),\n",
        "    seed=config.seed,\n",
        "    target_size=(config.img_width, config.img_height),\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3068 images belonging to 10 classes.\n",
            "Found 341 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M65p2WydS8w"
      },
      "source": [
        "def get_model(config):\n",
        "    # MobileNetV2 because it's the most lightweight model available in Keras API\n",
        "    # and I would prefer model size (also for inference) rather than complexity here.\n",
        "    # https://keras.io/api/applications/mobilenet/#mobilenetv2-function\n",
        "    mobileNet_base = MobileNetV2(\n",
        "        input_shape=(config.img_width, config.img_height, 3),\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "        classes=config.num_classes,\n",
        "        classifier_activation=\"softmax\",\n",
        "    )\n",
        "\n",
        "    mobileNet_base.trainable = False\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(mobileNet_base)\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.1))\n",
        "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "\n",
        "model = get_model(config)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(config.init_lr),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgVqAwle5eiD"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eltlHuhd_r9",
        "outputId": "e5598dae-a288-4109-d8e1-e2915e970198"
      },
      "source": [
        "lr_sched = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', min_delta=0.01, patience=10, factor=0.25, verbose=1)\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0.01, patience=20, verbose=1, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=config.num_epochs,\n",
        "    verbose=1,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[lr_sched, early_stop])\n",
        "\n",
        "model.save(config.checkpt_directory)\n",
        "!tar -czvf $config.checkpt_tar_path $config.checkpt_directory"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "48/48 [==============================] - 46s 965ms/step - loss: 0.3649 - accuracy: 0.8673 - val_loss: 0.6913 - val_accuracy: 0.7625\n",
            "Epoch 2/5\n",
            "48/48 [==============================] - 46s 952ms/step - loss: 0.3554 - accuracy: 0.8791 - val_loss: 0.5756 - val_accuracy: 0.8094\n",
            "Epoch 3/5\n",
            "48/48 [==============================] - 46s 954ms/step - loss: 0.3229 - accuracy: 0.8869 - val_loss: 0.6385 - val_accuracy: 0.7771\n",
            "Epoch 4/5\n",
            "48/48 [==============================] - 45s 948ms/step - loss: 0.3078 - accuracy: 0.8882 - val_loss: 0.5756 - val_accuracy: 0.8152\n",
            "Epoch 5/5\n",
            "48/48 [==============================] - 46s 949ms/step - loss: 0.2977 - accuracy: 0.8931 - val_loss: 0.5765 - val_accuracy: 0.8182\n",
            "INFO:tensorflow:Assets written to: /content/mobilenet_fashion_clothes_clf/assets\n",
            "tar: Removing leading `/' from member names\n",
            "/content/mobilenet_fashion_clothes_clf/\n",
            "/content/mobilenet_fashion_clothes_clf/saved_model.pb\n",
            "/content/mobilenet_fashion_clothes_clf/assets/\n",
            "/content/mobilenet_fashion_clothes_clf/variables/\n",
            "/content/mobilenet_fashion_clothes_clf/variables/variables.data-00000-of-00001\n",
            "/content/mobilenet_fashion_clothes_clf/variables/variables.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfgVDYTh5bwe"
      },
      "source": [
        "### Training History Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfCG9Pb3uPuH"
      },
      "source": [
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 4))\n",
        "\n",
        "axs[0].plot(history.history['loss'], label='loss')\n",
        "axs[0].plot(history.history['val_loss'], label='val_loss')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss')\n",
        "\n",
        "axs[1].plot(history.history['accuracy'], label='acc')\n",
        "axs[1].plot(history.history['val_accuracy'], label='val_acc')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "\n",
        "plt.legend();\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XqXB1Ix5YIc"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9HuiDc7xny-"
      },
      "source": [
        "inference_model = tf.keras.models.load_model(config.checkpts_directory)\n",
        "# print(inference_model)\n",
        "\n",
        "test_gen = ImageDataGenerator(preprocessing_function=preprocess_func)\n",
        "test_ds = train_gen.flow_from_directory(\n",
        "    os.path.join(dataset_directory, \"test/\"),\n",
        "    seed=config.seed,\n",
        "    target_size=(config.img_width, config.img_height),\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "y_pred = inference_model.predict(test_ds)\n",
        "y_pred = np.argmax(y_pred, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b82EHTMO5VA8"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHk9DKAQ3TBV"
      },
      "source": [
        "class_names = test_ds.class_indices\n",
        "class_names_list = list(class_names.keys())\n",
        "class_names = {v: k for k, v in class_names.items()}\n",
        "\n",
        "y_pred_labels = [class_names[x] for x in y_pred]\n",
        "y_true_labels = [class_names[x] for x in test_ds.classes]\n",
        "cm = confusion_matrix(y_pred_labels, y_true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNrqYqHk4YTL"
      },
      "source": [
        "plt.figure(figsize=(20, 6))\n",
        "ax = sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names_list, yticklabels=class_names_list)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}